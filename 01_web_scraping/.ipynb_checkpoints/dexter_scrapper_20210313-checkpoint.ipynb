{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Importing dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup #requires pip install\n",
    "import requests #requires pip install\n",
    "import re\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Scraper setup for dexters.co.uk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document time \n",
    "time_started = str(datetime.datetime.now()).replace(\" \",\"_\").replace(\":\",\"-\")[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define list of subway stations\n",
    "Underground_lines = ['Bakerloo', 'Central', 'Circle', 'District', 'DLR', 'Hammersmith & City',\n",
    "                     'Jubilee', 'Metropolitan', 'Northern', 'Piccadilly', 'Victoria', 'Waterloo & City']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Functions to scrape information from main add page, only related to price, address, and features of the add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to extract characteristics on each ad from the main webpage\n",
    "def feature_extract(html_text):\n",
    "    \n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    ## Parse for the different divisions within the add\n",
    "    ads = soup.find_all('div', class_ = 'result-content') #searches for 'div' and is filtered by the CSS-snippet\n",
    "\n",
    "    ## Set-up for the loop \n",
    "    results = {} #create nested dictionary to store the results\n",
    "    id_ad = 0 #insert ad_ID to distinguish between each ad \n",
    "\n",
    "    ## Loop across all ads\n",
    "    for k in range(len(ads)):\n",
    "        ad = ads[k]\n",
    "        id_ad += 1\n",
    "        results[id_ad] = {}\n",
    "\n",
    "        ## Extracting features from the ad\n",
    "        name = ad.find('h3').a.contents[0]\n",
    "        try:\n",
    "            price = ad.find('span', class_ = 'price-qualifier').text #catches the price WITHIN one ad\n",
    "        except:\n",
    "            continue\n",
    "        address = ad.find('span', class_ = 'address-area-post').text\n",
    "\n",
    "        # Number of bedrooms extracted from string\n",
    "        try:\n",
    "            bedrooms = ad.find('li', class_ = 'Bedrooms').text\n",
    "        except:\n",
    "            continue\n",
    "        bedrooms_nbr = int(bedrooms.split()[0])\n",
    "\n",
    "        # Number of bedrooms extracted from string\n",
    "        bathrooms_str = str(ad.find('li',class_ = 'Bathrooms'))\n",
    "        bathrooms_nbr = re.findall(r'\\d+', bathrooms_str)\n",
    "        bathrooms_nbr2 = int(bathrooms_nbr[0] if len(bathrooms_nbr)!= 0  else 0)\n",
    "\n",
    "        # Number of bedrooms extracted from string\n",
    "        reception_str = str(ad.find('li',class_ = 'Receptions'))\n",
    "        reception_nbr = re.findall(r'\\d+', reception_str)\n",
    "        reception_nbr2 = int(reception_nbr[0] if len(reception_nbr)!= 0  else 1)\n",
    "\n",
    "        link = ad.find('h3').a.get(\"href\")\n",
    "\n",
    "        # Create dictionary of results per ad id\n",
    "        results[id_ad][\"name\"] = name\n",
    "        results[id_ad][\"price\"] = price\n",
    "        results[id_ad][\"address\"] = address\n",
    "        results[id_ad][\"bedrooms\"] = bedrooms_nbr\n",
    "        results[id_ad][\"bathrooms\"] = bathrooms_nbr2\n",
    "        results[id_ad][\"reception\"] = reception_nbr2\n",
    "        results[id_ad][\"link\"] = (\"https://www.dexters.co.uk\" + link)\n",
    "        \n",
    "        # Create dataframe from dictionary of results\n",
    "        df_houses = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "    return df_houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create list of pages base on url and number of iterations desired\n",
    "def page_list(string, iterations):\n",
    "    pages_list = []\n",
    "    for i in range(iterations):\n",
    "        pages_list.append(string + str(i+1))\n",
    "        \n",
    "    return pages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get the maximum number of listing on Dexter's website\n",
    "def page_max(url):\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    amount = soup.find('span', class_ = 'marker-count has-results').text\n",
    "    amount_num = re.sub('\\D', '', amount)\n",
    "    return int(amount_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to launch scrapper on a specific webpage with number of pages to scrap\n",
    "def pages_scrap(main_page, iter_page, pages):\n",
    "    max_pages = (page_max(main_page)/18)\n",
    "    list_of_pages = page_list(iter_page, pages) # Create list of pages to scrape\n",
    "    df_list = [] #Create list of dataframes to be concatenated by the end of the loop\n",
    "    \n",
    "    # Loop through all pages to create the different dataframes\n",
    "    for page in list_of_pages:\n",
    "        html_page = requests.get(page)\n",
    "        html_page.encoding = 'utf-8'\n",
    "        page = html_page.text\n",
    "        df_ads = feature_extract(page)\n",
    "        df_list.append(df_ads)\n",
    "    \n",
    "    # Concatenate the different dataframes\n",
    "    df_results = pd.concat(df_list)\n",
    "    df_results = df_results.drop_duplicates()\n",
    "    df_results = df_results.reset_index(drop=True)\n",
    "    \n",
    "    print('Remaining number of page: ', int(max_pages - pages) )\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Subway related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to extract subway info list from a house webpage on dexter\n",
    "def get_info_subway(link):\n",
    "    html_text = requests.get(link).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    subway = soup.find('ul', class_ = 'list-information').text\n",
    "    \n",
    "    return subway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get list of values for subway distances with string\n",
    "\n",
    "def sub_values(string):\n",
    "    split = string.split('\\n')\n",
    "    list_1 = list(filter(None, split))\n",
    "    \n",
    "    list_2 = []\n",
    "    for i in list_1:\n",
    "        x = i.split('-')\n",
    "        list_2.append(x)\n",
    "\n",
    "    list_3 = [item.strip() for sublist in list_2 for item in sublist]\n",
    "    list_4 = list_3[0:3]\n",
    "    \n",
    "    return list_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get the closest stop on the tube if any\n",
    "def closest_line(list_of_lines):\n",
    "    j = 0\n",
    "    nearby_data = []\n",
    "    for i in range(len(list_of_lines)):\n",
    "        if list_of_lines[i] == 'London Underground' or list_of_lines[i] in Underground_lines and (j != 1 and i!=0):\n",
    "#             print(list_of_lines[i-2])\n",
    "            if (' ' in list_of_lines[i-2]) == False :\n",
    "                nearby_data.append(list_of_lines[i-3])\n",
    "                nearby_data.append(list_of_lines[i-2])\n",
    "                nearby_data.append(list_of_lines[i-1])\n",
    "                nearby_data.append(list_of_lines[i])\n",
    "                j = 1\n",
    "                \n",
    "                nearby_data[0] = (' '.join(nearby_data[0:2]))\n",
    "                del nearby_data[1]\n",
    "            \n",
    "            else:\n",
    "                nearby_data.append(list_of_lines[i-2])\n",
    "                nearby_data.append(list_of_lines[i-1])\n",
    "                nearby_data.append(list_of_lines[i])\n",
    "                j = 1\n",
    "\n",
    "    return nearby_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to populate datafrmae with closest tube stop name, distance, and related tube line\n",
    "\n",
    "def subway_per_house(df):\n",
    "    #Create new empty (NaN) columns in the existing dataframe\n",
    "    df = df.reindex(columns = df.columns.tolist() + ['tube_stop','tube_dist','tube_line'])\n",
    "    \n",
    "    #Loop through all lines of dataframe\n",
    "    for i in range(len(df)):\n",
    "        x = df['link'].iloc[i] #Get link of house page to scrape\n",
    "        subs = get_info_subway(x) #Extract tube line info\n",
    "        subs_2 = sub_values(subs) #Get list of subway station and distance\n",
    "        subs_3 = closest_line(subs_2) #Extract closest tube station only\n",
    "        \n",
    "        # Populate dataframe if a tubeway station has been found or not\n",
    "        if len(subs_3)!= 0:\n",
    "            df['tube_stop'].iloc[i] = subs_3[0]\n",
    "            df['tube_dist'].iloc[i] = subs_3[1]\n",
    "            df['tube_line'].iloc[i] = subs_3[2]\n",
    "        else:\n",
    "            df['tube_stop'].iloc[i] = np.NaN\n",
    "            df['tube_dist'].iloc[i] = np.NaN\n",
    "            df['tube_line'].iloc[i] = np.NaN\n",
    "    \n",
    "    df = df.astype(str)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to clean subway output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tube_dist(string):\n",
    "    string_m = string.split(' ')\n",
    "    num_val = string_m[-1]\n",
    "    \n",
    "    return num_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tube(string):\n",
    "    string_m = string.split(' ')\n",
    "    string_m = string_m[:-1]\n",
    "    string_m = ' '.join(string_m)\n",
    "    \n",
    "    return string_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to clean subway stops when too many words in the string\n",
    "\n",
    "def clean_tube_stop_string(string):\n",
    "    forbiddden_words = ['London Overground', 'Railway', 'Network Rail', 'Tramlink']\n",
    "    count_forbidden = 0\n",
    "\n",
    "    for j in forbiddden_words:\n",
    "        if count_forbidden == 0:\n",
    "            if j in string:\n",
    "                string_update = string.split()[-1]\n",
    "                count_forbidden = 1\n",
    "            else:\n",
    "                string_update = string\n",
    "            \n",
    "    return(string_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to input tube distance into the right column when value is in 'tube_stop'\n",
    "\n",
    "def clean_tube_dist(df):\n",
    "    df['tube_dist'] = df['tube_dist'].astype('str')\n",
    "    \n",
    "    errors  = df[df.loc[:, 'tube_dist'].map(hasNumbers) == False].copy()\n",
    "    errors_2 = errors.loc[errors['tube_stop'] != 'nan'].copy()\n",
    "    errors_2.loc[:, 'tube_dist'] = errors_2.loc[:, 'tube_stop'].map(get_tube_dist)\n",
    "    errors_2.loc[:, 'tube_stop'] = errors_2.loc[:, 'tube_stop'].map(strip_tube)\n",
    "    errors_2\n",
    "    \n",
    "    #Create copy of original df for modification\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # replace values in final df\n",
    "    for i in errors_2.index:\n",
    "        df_copy.loc[i] = errors_2.loc[i]\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to deals with Victoria tube stops (VIctoria being both a tube stop and a line)\n",
    "\n",
    "def victoria_clean_stop(string):\n",
    "    str_vic = 'Victoria'\n",
    "    str_check = string.split()\n",
    "    if str_check[0] == 'Victoria':\n",
    "        str_return = str_check[1]\n",
    "    else:\n",
    "        str_return = str_vic\n",
    "\n",
    "    return str_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tube_victoria(df):\n",
    "    df['tube_stop'] = df['tube_stop'].astype('str')\n",
    "    \n",
    "    errors  = df[df['tube_stop'].str.contains('Victoria')].copy()\n",
    "    \n",
    "    errors.loc[:, 'tube_stop'] = errors.loc[:, 'tube_stop'].map(victoria_clean_stop)\n",
    "\n",
    "    #Create copy of original df for modification\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # replace values in final df\n",
    "    for i in errors.index:\n",
    "        df_copy.loc[i] = errors.loc[i]\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final cleaning function to appy previous cleaning on 'tube_stop' and 'tube_dist' for the whole dataframe\n",
    "\n",
    "def clean_tube_stop(df):\n",
    "    df_2 = df.copy()\n",
    "    df_2 = clean_tube_dist(df_2)\n",
    "    df_2['tube_stop'] = df_2['tube_stop'].astype('str')\n",
    "    df_2['tube_stop'] = df_2['tube_stop'].map(clean_tube_stop_string)\n",
    "    \n",
    "    df_2 = clean_tube_victoria(df_2)\n",
    "    \n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Execution on a sample set of adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## pages_scrap(base_link, pages_link, #number of pages to scrap)\n",
    "\n",
    "\n",
    "dexter_list_1 = pages_scrap('https://www.dexters.co.uk/property-sales/properties-for-sale-in-london',\n",
    "                            'https://www.dexters.co.uk/property-sales/properties-for-sale-in-london/page-', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dexter_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Fetch subway related information from the previous dataframe\n",
    "output_list = subway_per_house(dexter_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned = clean_tube_stop(output_list)\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['tube_stop'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to file and reading from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.to_json(r'C:\\Users\\user\\Documents_C\\Python\\Jupyter\\05. Data engineering\\Dexter_output_clean.json', orient = 'columns')\n",
    "cleaned.to_csv(r'C:\\Users\\user\\Documents_C\\Python\\Jupyter\\05. Data engineering\\Dexter_output_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.read_json(r'C:\\Users\\user\\Documents_C\\Python\\Jupyter\\05. Data engineering\\Group_Project\\Dexter_output_v01.json',\n",
    "            orient = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['tube_stop'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
